"""Synthetic Central Bank Speeches Pipeline - Two-Stage Synthesis.

Generates privacy-preserving synthetic twin of the speeches dataset using:
1. NVIDIA Safe Synthesizer for summary-based synthesis (fits in context)
2. GPT-OSS 120B for expanding synthetic summaries into full speeches

Pipeline is DECOUPLED from Phase 3 - loads enriched data from LakeFS.

GPU Orchestration (Automatic via KAI):
1. NIM runs with 'inference' priority (125, non-preemptible)
2. Safe Synthesizer runs with 'batch-high' priority (130)
3. KAI preempts NIM pod when Safe Synth job starts
4. After Safe Synth completes, NIM Deployment restarts its pod
5. No manual kubectl commands required!
"""

import io
import json
from datetime import datetime, timezone
from typing import Any

import dagster as dg
import polars as pl

from brev_pipelines.config import PipelineConfig
from brev_pipelines.resources.lakefs import LakeFSResource
from brev_pipelines.resources.nim import NIMResource
from brev_pipelines.resources.nim_embedding import NIMEmbeddingResource
from brev_pipelines.resources.safe_synth import SafeSynthesizerResource
from brev_pipelines.resources.weaviate import WeaviateResource

# Weaviate schema for synthetic speeches
SYNTHETIC_SCHEMA: list[dict[str, str]] = [
    {"name": "speech_id", "type": "text", "description": "Unique identifier (SYNTH-XXXXXX)"},
    {"name": "date", "type": "text", "description": "Speech date (ISO format)"},
    {"name": "central_bank", "type": "text", "description": "Issuing institution"},
    {"name": "speaker", "type": "text", "description": "Speaker name"},
    {"name": "title", "type": "text", "description": "Speech title"},
    {"name": "text", "type": "text", "description": "Full speech text (GPT-OSS expanded)"},
    {"name": "summary", "type": "text", "description": "Speech summary"},
    {"name": "tariff_mention", "type": "boolean", "description": "Contains tariff discussion"},
    {"name": "is_governor", "type": "boolean", "description": "Speaker is central bank governor"},
    {"name": "is_synthetic", "type": "boolean", "description": "Synthetic data marker"},
]


@dg.asset(
    description="Load enriched speeches data product from LakeFS for synthesis",
    group_name="synthetic_speeches",
    metadata={
        "layer": "input",
        "source": "lakefs",
    },
)
def enriched_data_for_synthesis(
    context: dg.AssetExecutionContext,
    config: PipelineConfig,
    lakefs: LakeFSResource,
) -> pl.DataFrame:
    """Load enriched speeches data product from LakeFS.

    This DECOUPLES the synthetic pipeline from the ETL pipeline,
    allowing them to run independently. The ETL pipeline must complete
    first and store data in LakeFS before running synthesis.

    Args:
        context: Dagster execution context for logging.
        config: Pipeline configuration (is_trial for path selection).
        lakefs: LakeFS resource for data versioning.

    Returns:
        DataFrame with enriched speeches including summaries.
    """
    lakefs_client = lakefs.get_client()

    # Determine path based on trial mode
    if config.is_trial:
        path = "central-bank-speeches/trial/speeches.parquet"
        context.log.info("TRIAL RUN: Loading from trial-specific LakeFS path")
    else:
        path = "central-bank-speeches/speeches.parquet"

    context.log.info(f"Loading enriched speeches from lakefs://data/main/{path}")

    # Download from LakeFS
    response = lakefs_client.objects_api.get_object(
        repository="data",
        ref="main",
        path=path,
    )

    # Load as DataFrame (response is bytes directly)
    df = pl.read_parquet(io.BytesIO(response))
    context.log.info(f"Loaded {len(df)} enriched speeches from LakeFS")

    # Verify required columns exist
    required_columns = ["speech_id", "summary"]
    missing = [c for c in required_columns if c not in df.columns]
    if missing:
        raise ValueError(
            f"Missing required columns in LakeFS data: {missing}. "
            "Run the ETL pipeline (Phase 3) first to generate summaries."
        )

    return df


@dg.asset(
    description="Synthetic summaries generated by NVIDIA Safe Synthesizer",
    group_name="synthetic_speeches",
    metadata={
        "layer": "synthetic",
        "uses_gpu": "true",
        "gpu_orchestration": "KAI priority-based preemption",
    },
)
def synthetic_summaries(
    context: dg.AssetExecutionContext,
    enriched_data_for_synthesis: pl.DataFrame,
    safe_synth: SafeSynthesizerResource,
) -> tuple[pl.DataFrame, dict[str, Any]]:
    """Generate synthetic speech summaries using Safe Synthesizer.

    Trains on SUMMARIES (not full text) to fit within 2048 token context.
    Summaries are ~2000 tokens each, fitting 2-3 examples in the prompt.

    GPU orchestration is handled automatically by KAI Scheduler:
    - Safe Synthesizer job runs with 'batch-high' priority (130)
    - KAI preempts NIM (priority 125) to free the GPU
    - After job completion, NIM Deployment restarts automatically

    Args:
        context: Dagster execution context for logging.
        enriched_data_for_synthesis: Enriched speeches from LakeFS (with summaries).
        safe_synth: Safe Synthesizer resource for privacy-preserving synthesis.

    Returns:
        Tuple of (synthetic summaries DataFrame, evaluation report).
    """
    df = enriched_data_for_synthesis
    run_id = context.run_id or datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")

    context.log.info("Starting synthetic data generation with KAI GPU orchestration...")
    context.log.info("Training on SUMMARIES (not full text) to fit context window")
    context.log.info("KAI Scheduler will automatically preempt NIM to free the GPU")

    # Columns for synthesis - METADATA ONLY for reliable synthesis
    # Summary is too complex for TinyLlama to reproduce - we'll generate
    # new summaries with GPT-OSS during the expansion phase instead
    synthesis_columns = [
        "speech_id",
        "date",
        "central_bank",
        "speaker",
        "title",
        # REMOVED: "summary" - causes underfitting, generate during expansion
        "tariff_mention",
        "is_governor",
    ]

    # Select columns that exist
    available_columns = [c for c in synthesis_columns if c in df.columns]
    context.log.info(f"Synthesis columns: {available_columns}")

    df_for_synthesis = df.select(available_columns)

    # Convert to list of dicts for Safe Synthesizer
    data_for_synthesis = df_for_synthesis.to_dicts()

    context.log.info(f"Training on {len(data_for_synthesis)} records (metadata only, no summaries)")
    context.log.info(f"Summaries will be generated by GPT-OSS during speech expansion")

    # Single synthesis call with ALL data
    synthetic_data, evaluation = safe_synth.synthesize(
        input_data=data_for_synthesis,
        run_id=run_id,
        config={
            "epsilon": 6.0,  # Recommended 4-12 for large datasets
            "piiReplacement": True,
            "runMiaEvaluation": True,
            "runAiaEvaluation": True,
        },
    )

    # Convert to DataFrame
    synthetic_df = pl.DataFrame(synthetic_data)

    # Add synthetic marker and regenerate IDs
    synthetic_df = synthetic_df.with_row_index("_row_idx")
    synthetic_df = synthetic_df.with_columns(
        [
            (pl.lit("SYNTH-") + pl.col("_row_idx").cast(pl.Utf8).str.zfill(6)).alias(
                "speech_id"
            ),
            pl.lit(True).alias("is_synthetic"),
        ]
    )
    synthetic_df = synthetic_df.drop("_row_idx")

    combined_evaluation = {
        "total_records": len(synthetic_df),
        "mia_score": evaluation.get("mia_score") or 0,
        "aia_score": evaluation.get("aia_score") or 0,
        "quality_score": evaluation.get("quality_score") or 0,
        "privacy_passed": evaluation.get("privacy_passed", False),
        "job_id": evaluation.get("job_id", ""),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "gpu_orchestration": "KAI priority-based preemption",
        "synthesis_type": "summary-based",
    }

    context.log.info(f"Generated {len(synthetic_df)} synthetic summaries")
    context.log.info(f"Privacy passed: {combined_evaluation['privacy_passed']}")
    context.log.info(f"MIA score: {combined_evaluation['mia_score']}, AIA score: {combined_evaluation['aia_score']}")
    context.log.info("KAI Scheduler will restore NIM automatically for speech expansion")

    return (synthetic_df, combined_evaluation)


@dg.asset(
    description="Full synthetic speeches expanded from summaries by GPT-OSS 120B",
    group_name="synthetic_speeches",
    metadata={
        "layer": "enriched",
        "uses_gpu": "true",
        "model": "GPT-OSS 120B",
    },
)
def synthetic_speeches(
    context: dg.AssetExecutionContext,
    synthetic_summaries: tuple[pl.DataFrame, dict[str, Any]],
    nim: NIMResource,
) -> tuple[pl.DataFrame, dict[str, Any]]:
    """Expand synthetic summaries into full-length speeches using GPT-OSS.

    Takes the synthetic summaries generated by Safe Synthesizer and expands
    each into a realistic full-length central bank speech (~4000 chars).

    This step uses GPT-OSS 120B which is a large model - expect ~45-60 seconds
    per speech. For the full dataset (~7700 records), this takes ~100 hours.

    Args:
        context: Dagster execution context for logging.
        synthetic_summaries: Tuple of (synthetic summaries DataFrame, evaluation).
        nim: NIM resource configured for GPT-OSS model.

    Returns:
        Tuple of (expanded speeches DataFrame, evaluation report).
    """
    df, evaluation = synthetic_summaries
    total_records = len(df)

    context.log.info(f"Generating {total_records} full speeches from metadata...")
    context.log.info("Using GPT-OSS 120B - this will take time (~45s per speech)")

    expanded_texts: list[str] = []
    summaries: list[str] = []
    TARGET_LENGTH = 4000  # Target ~4000 chars per expanded speech

    for i, row in enumerate(df.iter_rows(named=True)):
        if (i + 1) % 10 == 0 or i == 0:
            context.log.info(f"Generating speech {i + 1}/{total_records}...")

        # Build generation prompt - note: no summary in input, we generate it fresh
        prompt = f"""You are a speechwriter for a central bank. Generate a realistic formal speech based on these metadata.

Central Bank: {row.get('central_bank', 'Unknown')}
Speaker: {row.get('speaker', 'Unknown')}
Title: {row.get('title', 'Untitled')}
Date: {row.get('date', '')}
Speaker is Governor: {row.get('is_governor', False)}
Topic includes tariff discussion: {row.get('tariff_mention', False)}

Generate a formal central bank speech (~4000 characters) that matches these metadata.
Use appropriate economic terminology and maintain a professional, measured tone.
Include:
- Opening remarks and acknowledgments
- Main policy discussion with specific details relevant to the title
- Economic outlook and projections
- Concluding statements and forward guidance

Speech:"""

        # Generate speech with longer timeout for 120B model
        expanded = nim.generate(
            prompt,
            max_tokens=1500,  # ~6000 chars
            temperature=0.7,
            timeout_override=300,  # 5 minutes for large model
        )

        # Check for errors
        if expanded.startswith("LLM error:"):
            context.log.warning(f"Speech {i + 1} generation failed: {expanded}")
            expanded = f"Speech by {row.get('speaker', 'Unknown')} on {row.get('title', 'monetary policy')}."

        # Enforce length limit
        expanded_texts.append(expanded[:5000])

        # Generate summary for the generated speech
        summary_prompt = f"""Summarize this central bank speech in 2-3 paragraphs (~500 words):

{expanded[:3000]}

Summary:"""

        summary = nim.generate(
            summary_prompt,
            max_tokens=500,
            temperature=0.3,
            timeout_override=120,
        )

        if summary.startswith("LLM error:"):
            summary = f"Speech by {row.get('speaker', 'Unknown')} discussing {row.get('title', 'monetary policy')}."

        summaries.append(summary[:2000])

    # Add text and summary columns (both generated by GPT-OSS)
    df = df.with_columns([
        pl.Series("text", expanded_texts),
        pl.Series("summary", summaries),
    ])

    # Update evaluation with expansion info
    evaluation_with_expansion = {
        **evaluation,
        "generation_model": "GPT-OSS 120B",
        "generation_target_length": TARGET_LENGTH,
        "generation_completed_at": datetime.now(timezone.utc).isoformat(),
    }

    context.log.info(f"Generated {len(expanded_texts)} speeches and summaries")
    avg_length = sum(len(t) for t in expanded_texts) / len(expanded_texts) if expanded_texts else 0
    context.log.info(f"Average speech length: {avg_length:.0f} chars")

    return (df, evaluation_with_expansion)


@dg.asset(
    description="Privacy validation report stored in LakeFS",
    group_name="synthetic_speeches",
    metadata={
        "layer": "validation",
        "destination": "lakefs",
    },
)
def synthetic_validation_report(
    context: dg.AssetExecutionContext,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    lakefs: LakeFSResource,
) -> dict[str, Any]:
    """Store privacy validation report in LakeFS.

    Contains MIA (Membership Inference Attack) and AIA (Attribute Inference Attack)
    evaluation scores to verify synthetic data privacy.

    Args:
        context: Dagster execution context for logging.
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation report).
        lakefs: LakeFS resource for data versioning.

    Returns:
        Validation report dictionary with privacy metrics.
    """
    from lakefs_sdk.models import CommitCreation

    _, evaluation = synthetic_speeches

    # Add metadata
    report = {
        **evaluation,
        "report_version": "2.0",  # Version 2.0 for two-stage synthesis
        "report_type": "safe-synthesizer-evaluation",
        "pipeline_type": "two-stage-synthesis",
    }

    # Store in LakeFS
    lakefs_client = lakefs.get_client()

    report_path = "central-bank-speeches/synthetic/validation_report.json"
    report_bytes = json.dumps(report, indent=2).encode()

    lakefs_client.objects_api.upload_object(
        repository="data",
        branch="main",
        path=report_path,
        content=report_bytes,
    )

    # Commit (skip if no changes)
    try:
        lakefs_client.commits_api.commit(
            repository="data",
            branch="main",
            commit_creation=CommitCreation(
                message="Add synthetic data validation report (two-stage synthesis)",
                metadata={
                    "dagster_run_id": context.run_id or "",
                    "mia_score": str(report.get("mia_score", "")),
                    "aia_score": str(report.get("aia_score", "")),
                    "privacy_passed": str(report.get("privacy_passed", "")),
                    "synthesis_type": "two-stage",
                },
            ),
        )
    except Exception as e:
        if "no changes" in str(e).lower():
            context.log.info("No changes to commit (report already exists in LakeFS)")
        else:
            raise

    context.log.info(f"Stored validation report to lakefs://data/main/{report_path}")

    return report


@dg.asset(
    description="Embeddings for synthetic speeches",
    group_name="synthetic_speeches",
    metadata={
        "layer": "enriched",
        "uses_nim_embedding": "true",
    },
)
def synthetic_embeddings(
    context: dg.AssetExecutionContext,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    nim_embedding: NIMEmbeddingResource,
) -> tuple[pl.DataFrame, list[list[float]]]:
    """Generate embeddings for synthetic speeches.

    Uses local NIM embedding model. Runs after speech expansion completes.

    Args:
        context: Dagster execution context for logging.
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation).
        nim_embedding: NIM embedding resource for vector generation.

    Returns:
        Tuple of (DataFrame, list of 1024-dim embedding vectors).
    """
    df, _ = synthetic_speeches

    # Prepare texts for embedding - use expanded text
    texts: list[str] = []
    for row in df.iter_rows(named=True):
        title = row.get("title", "") or ""
        text = row.get("text", "") or ""
        combined = f"{title}\n\n{text[:2000]}"
        texts.append(combined)

    context.log.info(f"Generating embeddings for {len(texts)} synthetic speeches...")

    # Generate embeddings (uses NVIDIA NIM embedding model)
    embeddings = nim_embedding.embed_texts(texts, batch_size=32)

    context.log.info(f"Generated {len(embeddings)} embeddings, dimension: {len(embeddings[0])}")

    return (df, embeddings)


@dg.asset(
    description="Synthetic speeches data product in LakeFS",
    group_name="synthetic_speeches",
    metadata={
        "layer": "output",
        "destination": "lakefs",
    },
)
def synthetic_data_product(
    context: dg.AssetExecutionContext,
    config: PipelineConfig,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    lakefs: LakeFSResource,
) -> dict[str, Any]:
    """Store synthetic speeches as versioned data product in LakeFS.

    Uses trial-specific path when is_trial=True to keep trial data separate.

    Args:
        context: Dagster execution context for logging.
        config: Pipeline configuration (is_trial for path selection).
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation).
        lakefs: LakeFS resource for data versioning.

    Returns:
        Dictionary with storage metadata (path, commit_id, counts).
    """
    from lakefs_sdk.models import CommitCreation

    df, _ = synthetic_speeches

    # Add timestamp
    df = df.with_columns(
        pl.lit(datetime.now(timezone.utc).isoformat()).alias("generated_at")
    )

    # Serialize to Parquet
    buffer = io.BytesIO()
    df.write_parquet(buffer)
    parquet_bytes = buffer.getvalue()

    # Store in LakeFS - use trial path if is_trial
    lakefs_client = lakefs.get_client()
    if config.is_trial:
        path = "central-bank-speeches/synthetic/trial/speeches.parquet"
        context.log.info("TRIAL RUN: Using trial-specific LakeFS path for synthetic data")
    else:
        path = "central-bank-speeches/synthetic/speeches.parquet"

    lakefs_client.objects_api.upload_object(
        repository="data",
        branch="main",
        path=path,
        content=parquet_bytes,
    )

    commit_id = None
    try:
        commit = lakefs_client.commits_api.commit(
            repository="data",
            branch="main",
            commit_creation=CommitCreation(
                message=f"Update synthetic speeches data product ({len(df)} records, two-stage synthesis)",
                metadata={
                    "dagster_run_id": context.run_id or "",
                    "num_records": str(len(df)),
                    "is_synthetic": "true",
                    "synthesis_type": "two-stage",
                },
            ),
        )
        commit_id = commit.id
        context.log.info(f"Committed synthetic data to LakeFS: {commit_id}")
    except Exception as e:
        if "no changes" in str(e).lower():
            context.log.info("No changes to commit (data already exists in LakeFS)")
        else:
            raise

    return {
        "path": f"lakefs://data/main/{path}",
        "commit_id": commit_id,
        "num_records": len(df),
    }


@dg.asset(
    description="Synthetic speeches indexed in Weaviate",
    group_name="synthetic_speeches",
    metadata={
        "layer": "output",
        "destination": "weaviate",
    },
)
def synthetic_weaviate_index(
    context: dg.AssetExecutionContext,
    config: PipelineConfig,
    synthetic_embeddings: tuple[pl.DataFrame, list[list[float]]],
    weaviate: WeaviateResource,
) -> dict[str, Any]:
    """Index synthetic speeches in separate Weaviate collection.

    Creates SyntheticSpeeches collection (separate from CentralBankSpeeches)
    per INV-P004 (Synthetic Data Isolation).
    Uses trial-specific collection when is_trial=True.

    Args:
        context: Dagster execution context for logging.
        config: Pipeline configuration (is_trial for collection selection).
        synthetic_embeddings: Tuple of (DataFrame, embeddings) from embedding step.
        weaviate: Weaviate resource for vector storage.

    Returns:
        Dictionary with indexing metadata (collection, count, dimensions).
    """
    df, embeddings = synthetic_embeddings

    # Use trial collection if is_trial
    if config.is_trial:
        collection_name = "SyntheticSpeechesTrial"
        context.log.info("TRIAL RUN: Using trial-specific Weaviate collection for synthetic data")
    else:
        collection_name = "SyntheticSpeeches"

    # Ensure collection exists with updated schema
    weaviate.ensure_collection(
        name=collection_name,
        properties=SYNTHETIC_SCHEMA,
        vector_dimensions=len(embeddings[0]),
    )

    # Prepare objects with all fields including summary
    objects: list[dict[str, Any]] = []
    for row in df.iter_rows(named=True):
        objects.append(
            {
                "speech_id": row["speech_id"],
                "date": str(row.get("date", "")),
                "central_bank": row.get("central_bank", "Unknown"),
                "speaker": row.get("speaker", "Unknown"),
                "title": row.get("title", "Untitled"),
                "text": (row.get("text", "") or "")[:10000],
                "summary": (row.get("summary", "") or "")[:5000],
                "tariff_mention": bool(row.get("tariff_mention", 0)),
                "is_governor": bool(row.get("is_governor", False)),
                "is_synthetic": True,
            }
        )

    # Insert objects with embeddings
    count = weaviate.insert_objects(
        collection_name=collection_name,
        objects=objects,
        vectors=embeddings,
    )

    context.log.info(f"Indexed {count} synthetic speeches in Weaviate collection: {collection_name}")

    return {
        "collection": collection_name,
        "object_count": count,
        "vector_dimensions": len(embeddings[0]),
    }


# Export all synthetic speech assets
synthetic_speeches_assets = [
    enriched_data_for_synthesis,
    synthetic_summaries,
    synthetic_speeches,
    synthetic_validation_report,
    synthetic_embeddings,
    synthetic_data_product,
    synthetic_weaviate_index,
]
