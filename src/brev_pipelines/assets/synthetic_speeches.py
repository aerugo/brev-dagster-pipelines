"""Synthetic Central Bank Speeches Pipeline.

Generates privacy-preserving synthetic twin of the speeches dataset
using NVIDIA Safe Synthesizer with KAI Scheduler integration.

GPU Orchestration (Automatic via KAI):
1. NIM runs with 'inference' priority (125, non-preemptible)
2. Safe Synthesizer runs with 'batch-high' priority (130)
3. KAI preempts NIM pod when Safe Synth job starts
4. After Safe Synth completes, NIM Deployment restarts its pod
5. No manual kubectl commands required!
"""

import io
import json
from datetime import datetime, timezone
from typing import Any

import dagster as dg
import polars as pl

from brev_pipelines.config import PipelineConfig
from brev_pipelines.resources.lakefs import LakeFSResource
from brev_pipelines.resources.nim_embedding import NIMEmbeddingResource
from brev_pipelines.resources.safe_synth import SafeSynthesizerResource
from brev_pipelines.resources.weaviate import WeaviateResource

# Weaviate schema for synthetic speeches
SYNTHETIC_SCHEMA: list[dict[str, str]] = [
    {"name": "speech_id", "type": "text", "description": "Unique identifier (SYNTH-XXXXXX)"},
    {"name": "date", "type": "text", "description": "Speech date (ISO format)"},
    {"name": "central_bank", "type": "text", "description": "Issuing institution"},
    {"name": "speaker", "type": "text", "description": "Speaker name"},
    {"name": "title", "type": "text", "description": "Speech title"},
    {"name": "text", "type": "text", "description": "Full speech text"},
    {"name": "tariff_mention", "type": "boolean", "description": "Contains tariff discussion"},
    {"name": "is_synthetic", "type": "boolean", "description": "Synthetic data marker"},
]


@dg.asset(
    description="Synthetic speeches generated by NVIDIA Safe Synthesizer",
    group_name="synthetic_speeches",
    metadata={
        "layer": "synthetic",
        "uses_gpu": "true",
        "gpu_orchestration": "KAI priority-based preemption",
    },
)
def synthetic_speeches(
    context: dg.AssetExecutionContext,
    enriched_speeches: pl.DataFrame,
    safe_synth: SafeSynthesizerResource,
) -> tuple[pl.DataFrame, dict[str, Any]]:
    """Generate synthetic twin of the speeches dataset.

    GPU orchestration is handled automatically by KAI Scheduler:
    - Safe Synthesizer job runs with 'batch-high' priority (130)
    - KAI preempts NIM (priority 125) to free the GPU
    - After job completion, NIM Deployment restarts automatically

    No manual intervention required!

    Args:
        context: Dagster execution context for logging.
        enriched_speeches: Enriched speeches DataFrame from Phase 3 pipeline.
        safe_synth: Safe Synthesizer resource for privacy-preserving synthesis.

    Returns:
        Tuple of (synthetic DataFrame, evaluation report).
    """
    df = enriched_speeches
    run_id = context.run_id or datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")

    context.log.info("Starting synthetic data generation with KAI GPU orchestration...")
    context.log.info("KAI Scheduler will automatically preempt NIM to free the GPU")

    # Prepare data for synthesis (exclude embeddings, they'll be regenerated)
    synthesis_columns = [
        "speech_id",
        "date",
        "central_bank",
        "speaker",
        "title",
        "text",
        "tariff_mention",
    ]

    # Select columns that exist
    available_columns = [c for c in synthesis_columns if c in df.columns]
    df_for_synthesis = df.select(available_columns)

    # Convert to list of dicts for Safe Synthesizer
    data_for_synthesis = df_for_synthesis.to_dicts()

    # Truncate long texts to fit TinyLlama context window
    # NOTE: rope_scaling_factor only applies to training, not vLLM generation!
    # vLLM generation uses max_model_len=2048 (TinyLlama base) regardless of RoPE config.
    # 2048 tokens â‰ˆ 8000 chars total, minus ~500 tokens for non-text fields = ~6000 chars
    # Using 4000 chars for safety margin to avoid Invalid JSON generation errors.
    # See docs/reports/safe-synthesizer-best-practices.md for context analysis.
    MAX_TEXT_LENGTH = 4000
    for record in data_for_synthesis:
        if "text" in record and record["text"] and len(record["text"]) > MAX_TEXT_LENGTH:
            record["text"] = record["text"][:MAX_TEXT_LENGTH] + "..."

    context.log.info(f"Training on {len(data_for_synthesis)} records (single run, no batching)")
    context.log.info(f"Text truncated to {MAX_TEXT_LENGTH} chars (fits in vLLM 2048 token context)")

    # Single synthesis call with ALL data (not batched!)
    # Safe Synthesizer trains once, then generates num_records synthetic records
    # See docs/reports/safe-synthesizer-best-practices.md for rationale
    synthetic_data, evaluation = safe_synth.synthesize(
        input_data=data_for_synthesis,
        run_id=run_id,
        config={
            "epsilon": 6.0,  # Recommended 4-12 for large datasets (>1000 records)
            "piiReplacement": True,
            "runMiaEvaluation": True,
            "runAiaEvaluation": True,
        },
    )

    # Convert to DataFrame
    synthetic_df = pl.DataFrame(synthetic_data)

    # Add synthetic marker and regenerate IDs using correct Polars pattern
    synthetic_df = synthetic_df.with_row_index("_row_idx")
    synthetic_df = synthetic_df.with_columns(
        [
            (pl.lit("SYNTH-") + pl.col("_row_idx").cast(pl.Utf8).str.zfill(6)).alias(
                "speech_id"
            ),
            pl.lit(True).alias("is_synthetic"),
        ]
    )
    synthetic_df = synthetic_df.drop("_row_idx")

    # Single evaluation result (no aggregation needed)
    combined_evaluation = {
        "total_records": len(synthetic_df),
        "mia_score": evaluation.get("mia_score") or 0,
        "aia_score": evaluation.get("aia_score") or 0,
        "quality_score": evaluation.get("quality_score") or 0,
        "privacy_passed": evaluation.get("privacy_passed", False),
        "job_id": evaluation.get("job_id", ""),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "gpu_orchestration": "KAI priority-based preemption",
    }

    context.log.info(f"Generated {len(synthetic_df)} synthetic speeches")
    context.log.info(f"Privacy passed: {combined_evaluation['privacy_passed']}")
    context.log.info(f"MIA score: {combined_evaluation['mia_score']}, AIA score: {combined_evaluation['aia_score']}")
    context.log.info("KAI Scheduler will restore NIM automatically")

    return (synthetic_df, combined_evaluation)


@dg.asset(
    description="Privacy validation report stored in LakeFS",
    group_name="synthetic_speeches",
    metadata={
        "layer": "validation",
        "destination": "lakefs",
    },
)
def synthetic_validation_report(
    context: dg.AssetExecutionContext,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    lakefs: LakeFSResource,
) -> dict[str, Any]:
    """Store privacy validation report in LakeFS.

    Contains MIA (Membership Inference Attack) and AIA (Attribute Inference Attack)
    evaluation scores to verify synthetic data privacy.

    Args:
        context: Dagster execution context for logging.
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation report).
        lakefs: LakeFS resource for data versioning.

    Returns:
        Validation report dictionary with privacy metrics.
    """
    from lakefs_sdk.models import CommitCreation

    _, evaluation = synthetic_speeches

    # Add metadata
    report = {
        **evaluation,
        "report_version": "1.0",
        "report_type": "safe-synthesizer-evaluation",
    }

    # Store in LakeFS
    lakefs_client = lakefs.get_client()

    report_path = "central-bank-speeches/synthetic/validation_report.json"
    report_bytes = json.dumps(report, indent=2).encode()

    lakefs_client.objects_api.upload_object(
        repository="data",
        branch="main",
        path=report_path,
        content=report_bytes,
    )

    # Commit (skip if no changes)
    try:
        lakefs_client.commits_api.commit(
            repository="data",
            branch="main",
            commit_creation=CommitCreation(
                message="Add synthetic data validation report",
                metadata={
                    "dagster_run_id": context.run_id or "",
                    "mia_score": str(report.get("mia_score", "")),
                    "aia_score": str(report.get("aia_score", "")),
                    "privacy_passed": str(report.get("privacy_passed", "")),
                },
            ),
        )
    except Exception as e:
        if "no changes" in str(e).lower():
            context.log.info("No changes to commit (report already exists in LakeFS)")
        else:
            raise

    context.log.info(f"Stored validation report to lakefs://data/main/{report_path}")

    return report


@dg.asset(
    description="Embeddings for synthetic speeches",
    group_name="synthetic_speeches",
    metadata={
        "layer": "enriched",
        "uses_nim_embedding": "true",
    },
)
def synthetic_embeddings(
    context: dg.AssetExecutionContext,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    nim_embedding: NIMEmbeddingResource,
) -> tuple[pl.DataFrame, list[list[float]]]:
    """Generate embeddings for synthetic speeches.

    Uses local NIM embedding model. Runs after Safe Synth completes
    and NIM is restored by KAI Scheduler.

    Args:
        context: Dagster execution context for logging.
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation).
        nim_embedding: NIM embedding resource for vector generation.

    Returns:
        Tuple of (DataFrame, list of 1024-dim embedding vectors).
    """
    df, _ = synthetic_speeches

    # Prepare texts for embedding
    texts: list[str] = []
    for row in df.iter_rows(named=True):
        title = row.get("title", "") or ""
        text = row.get("text", "") or ""
        combined = f"{title}\n\n{text[:2000]}"
        texts.append(combined)

    context.log.info(f"Generating embeddings for {len(texts)} synthetic speeches...")

    # Generate embeddings (uses NVIDIA NIM embedding model)
    embeddings = nim_embedding.embed_texts(texts, batch_size=32)

    context.log.info(f"Generated {len(embeddings)} embeddings, dimension: {len(embeddings[0])}")

    return (df, embeddings)


@dg.asset(
    description="Synthetic speeches data product in LakeFS",
    group_name="synthetic_speeches",
    metadata={
        "layer": "output",
        "destination": "lakefs",
    },
)
def synthetic_data_product(
    context: dg.AssetExecutionContext,
    config: PipelineConfig,
    synthetic_speeches: tuple[pl.DataFrame, dict[str, Any]],
    lakefs: LakeFSResource,
) -> dict[str, Any]:
    """Store synthetic speeches as versioned data product in LakeFS.

    Uses trial-specific path when is_trial=True to keep trial data separate.

    Args:
        context: Dagster execution context for logging.
        config: Pipeline configuration (is_trial for path selection).
        synthetic_speeches: Tuple of (synthetic DataFrame, evaluation).
        lakefs: LakeFS resource for data versioning.

    Returns:
        Dictionary with storage metadata (path, commit_id, counts).
    """
    from lakefs_sdk.models import CommitCreation

    df, _ = synthetic_speeches

    # Add timestamp
    df = df.with_columns(
        pl.lit(datetime.now(timezone.utc).isoformat()).alias("generated_at")
    )

    # Serialize to Parquet
    buffer = io.BytesIO()
    df.write_parquet(buffer)
    parquet_bytes = buffer.getvalue()

    # Store in LakeFS - use trial path if is_trial
    lakefs_client = lakefs.get_client()
    if config.is_trial:
        path = "central-bank-speeches/synthetic/trial/speeches.parquet"
        context.log.info("TRIAL RUN: Using trial-specific LakeFS path for synthetic data")
    else:
        path = "central-bank-speeches/synthetic/speeches.parquet"

    lakefs_client.objects_api.upload_object(
        repository="data",
        branch="main",
        path=path,
        content=parquet_bytes,
    )

    commit_id = None
    try:
        commit = lakefs_client.commits_api.commit(
            repository="data",
            branch="main",
            commit_creation=CommitCreation(
                message=f"Update synthetic speeches data product ({len(df)} records)",
                metadata={
                    "dagster_run_id": context.run_id or "",
                    "num_records": str(len(df)),
                    "is_synthetic": "true",
                },
            ),
        )
        commit_id = commit.id
        context.log.info(f"Committed synthetic data to LakeFS: {commit_id}")
    except Exception as e:
        if "no changes" in str(e).lower():
            context.log.info("No changes to commit (data already exists in LakeFS)")
        else:
            raise

    return {
        "path": f"lakefs://data/main/{path}",
        "commit_id": commit_id,
        "num_records": len(df),
    }


@dg.asset(
    description="Synthetic speeches indexed in Weaviate",
    group_name="synthetic_speeches",
    metadata={
        "layer": "output",
        "destination": "weaviate",
    },
)
def synthetic_weaviate_index(
    context: dg.AssetExecutionContext,
    config: PipelineConfig,
    synthetic_embeddings: tuple[pl.DataFrame, list[list[float]]],
    weaviate: WeaviateResource,
) -> dict[str, Any]:
    """Index synthetic speeches in separate Weaviate collection.

    Creates SyntheticSpeeches collection (separate from CentralBankSpeeches)
    per INV-P004 (Synthetic Data Isolation).
    Uses trial-specific collection when is_trial=True.

    Args:
        context: Dagster execution context for logging.
        config: Pipeline configuration (is_trial for collection selection).
        synthetic_embeddings: Tuple of (DataFrame, embeddings) from embedding step.
        weaviate: Weaviate resource for vector storage.

    Returns:
        Dictionary with indexing metadata (collection, count, dimensions).
    """
    df, embeddings = synthetic_embeddings

    # Use trial collection if is_trial
    if config.is_trial:
        collection_name = "SyntheticSpeechesTrial"
        context.log.info("TRIAL RUN: Using trial-specific Weaviate collection for synthetic data")
    else:
        collection_name = "SyntheticSpeeches"

    # Ensure collection exists
    weaviate.ensure_collection(
        name=collection_name,
        properties=SYNTHETIC_SCHEMA,
        vector_dimensions=len(embeddings[0]),
    )

    # Prepare objects
    objects: list[dict[str, Any]] = []
    for row in df.iter_rows(named=True):
        objects.append(
            {
                "speech_id": row["speech_id"],
                "date": str(row.get("date", "")),
                "central_bank": row.get("central_bank", "Unknown"),
                "speaker": row.get("speaker", "Unknown"),
                "title": row.get("title", "Untitled"),
                "text": (row.get("text", "") or "")[:10000],
                "tariff_mention": bool(row.get("tariff_mention", 0)),
                "is_synthetic": True,
            }
        )

    # Insert objects with embeddings
    count = weaviate.insert_objects(
        collection_name=collection_name,
        objects=objects,
        vectors=embeddings,
    )

    context.log.info(f"Indexed {count} synthetic speeches in Weaviate collection: {collection_name}")

    return {
        "collection": collection_name,
        "object_count": count,
        "vector_dimensions": len(embeddings[0]),
    }


# Export all synthetic speech assets
synthetic_speeches_assets = [
    synthetic_speeches,
    synthetic_validation_report,
    synthetic_embeddings,
    synthetic_data_product,
    synthetic_weaviate_index,
]
