# Dagster Pipeline Development Best Practices

This document defines coding standards for all Dagster pipeline code in the Brev Data Platform. These guidelines ensure consistency, maintainability, and type safety.

## Core Principles

1. **Strict Type Hints**: All functions, assets, and resources must have complete type annotations
2. **Pydantic for Data Models**: Use Pydantic v2 API for all structured data
3. **PydanticAI for LLM Steps**: Use PydanticAI for LLM-based processing with structured outputs
4. **Composition over Inheritance**: Build functionality through composition, not class hierarchies
5. **Comprehensive Docstrings**: All public functions and classes must have docstrings

---

## Type Annotations

### Required Annotations

Every function must have complete type annotations for parameters and return values.

```python
# GOOD: Complete type annotations
def process_speeches(
    speeches: list[Speech],
    batch_size: int = 32,
) -> tuple[pl.DataFrame, list[EmbeddingResult]]:
    ...

# BAD: Missing annotations
def process_speeches(speeches, batch_size=32):
    ...
```

### Use Modern Python Typing

Use Python 3.10+ typing syntax:

```python
# GOOD: Modern syntax
from collections.abc import Sequence

def get_items(ids: list[str]) -> dict[str, Item | None]:
    ...

def process(data: Sequence[Record]) -> list[Result]:
    ...

# BAD: Legacy typing module
from typing import List, Dict, Optional, Union

def get_items(ids: List[str]) -> Dict[str, Optional[Item]]:
    ...
```

### Type Aliases for Complex Types

Define type aliases for complex or repeated types:

```python
# Define at module level
EmbeddingVector = list[float]
BatchResult = tuple[pl.DataFrame, list[EmbeddingVector]]
SpeechRecord = dict[str, str | int | float | None]

@dg.asset
def speech_embeddings(
    speeches: pl.DataFrame,
    nim_embedding: NIMEmbeddingResource,
) -> BatchResult:
    ...
```

---

## Pydantic Models (v2 API)

### Model Definition

Use Pydantic v2 for all structured data with proper field definitions:

```python
from pydantic import BaseModel, Field, field_validator
from datetime import date

class Speech(BaseModel):
    """A central bank speech record."""

    speech_id: str = Field(..., description="Unique identifier for the speech")
    title: str = Field(..., min_length=1, description="Speech title")
    date: date = Field(..., description="Date of the speech")
    central_bank: str = Field(..., description="Issuing central bank")
    speaker: str | None = Field(default=None, description="Speaker name if known")
    text: str = Field(..., min_length=10, description="Full speech text")
    tariff_mention: bool = Field(default=False, description="Whether speech mentions tariffs")

    @field_validator("central_bank")
    @classmethod
    def validate_central_bank(cls, v: str) -> str:
        """Normalize central bank names."""
        return v.strip().upper()

class SpeechCollection(BaseModel):
    """Collection of speeches with metadata."""

    speeches: list[Speech]
    source: str = Field(..., description="Data source identifier")
    extracted_at: datetime = Field(default_factory=datetime.utcnow)

    @property
    def count(self) -> int:
        return len(self.speeches)
```

### Dagster Resources with Pydantic

Use `ConfigurableResource` (Pydantic-based) for Dagster resources:

```python
from dagster import ConfigurableResource
from pydantic import Field, SecretStr

class NIMEmbeddingResource(ConfigurableResource):
    """Resource for generating embeddings via local NIM service."""

    endpoint: str = Field(
        default="http://nvidia-nim-embedding.nvidia-nim.svc.cluster.local:8000",
        description="NIM embedding service endpoint",
    )
    model: str = Field(
        default="nvidia/llama-3_2-nemoretriever-300m-embed-v2",
        description="Embedding model name",
    )
    timeout: int = Field(default=30, ge=1, le=300, description="Request timeout in seconds")
    batch_size: int = Field(default=32, ge=1, le=256, description="Batch size for embedding requests")

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of text strings to embed.

        Returns:
            List of embedding vectors (1024 dimensions each).

        Raises:
            EmbeddingError: If the embedding request fails.
        """
        ...
```

### Validation and Serialization

Use Pydantic for input validation and output serialization:

```python
class ClassificationResult(BaseModel):
    """Result of LLM classification."""

    speech_id: str
    classification: Literal["tariff_related", "not_tariff_related"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str | None = None

    model_config = ConfigDict(
        # Pydantic v2 config
        strict=True,
        frozen=True,  # Immutable after creation
    )

def parse_llm_response(response: str, speech_id: str) -> ClassificationResult:
    """Parse and validate LLM classification response."""
    # Pydantic validates automatically
    return ClassificationResult.model_validate_json(response)
```

---

## PydanticAI for LLM Processing

### Why PydanticAI

Use PydanticAI for LLM steps because it provides:
- Structured output validation with Pydantic models
- Automatic retries on validation failures
- Type-safe prompt templates
- Built-in support for multiple LLM providers

### Basic PydanticAI Usage

```python
from pydantic import BaseModel, Field
from pydantic_ai import Agent

class TariffClassification(BaseModel):
    """Classification result for tariff mention detection."""

    mentions_tariff: bool = Field(description="Whether the speech mentions tariffs or trade policy")
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence score")
    evidence: list[str] = Field(default_factory=list, description="Relevant quotes from text")

# Create a typed agent
tariff_classifier = Agent(
    model="openai:gpt-4o-mini",  # Or use NIM endpoint
    result_type=TariffClassification,
    system_prompt=(
        "You are an expert analyst classifying central bank speeches. "
        "Determine if the speech discusses tariffs, trade policy, or trade tensions. "
        "Extract relevant evidence quotes."
    ),
)

async def classify_speech(text: str) -> TariffClassification:
    """Classify a speech for tariff mentions using PydanticAI."""
    result = await tariff_classifier.run(
        f"Classify this speech:\n\n{text[:4000]}"
    )
    return result.data
```

### PydanticAI with Local NIM

Configure PydanticAI to use local NIM endpoint:

```python
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel

# Configure for local NIM (OpenAI-compatible API)
nim_model = OpenAIModel(
    model_name="meta/llama3-8b-instruct",
    base_url="http://nvidia-nim.nvidia-nim.svc.cluster.local:8000/v1",
    api_key="not-required",  # NIM doesn't require API key
)

class SpeechSummary(BaseModel):
    """Structured summary of a speech."""

    main_topics: list[str] = Field(description="Main topics discussed")
    sentiment: Literal["hawkish", "dovish", "neutral"] = Field(description="Monetary policy sentiment")
    key_quotes: list[str] = Field(max_length=3, description="Most important quotes")

summarizer = Agent(
    model=nim_model,
    result_type=SpeechSummary,
    system_prompt="Summarize central bank speeches with focus on monetary policy stance.",
)
```

### PydanticAI in Dagster Assets

Integrate PydanticAI with Dagster assets:

```python
import dagster as dg
from pydantic_ai import Agent

class ClassificationOutput(BaseModel):
    """Output model for classification asset."""

    speech_id: str
    classification: TariffClassification
    model_version: str

@dg.asset(
    description="Classify speeches for tariff mentions using PydanticAI",
    group_name="central_bank_speeches",
)
async def tariff_classifications(
    context: dg.AssetExecutionContext,
    cleaned_speeches: pl.DataFrame,
) -> list[ClassificationOutput]:
    """Classify all speeches using PydanticAI agent.

    Uses structured output to ensure consistent classification format.
    """
    results: list[ClassificationOutput] = []

    for row in cleaned_speeches.iter_rows(named=True):
        speech_id = row["speech_id"]
        text = row["text"]

        try:
            result = await tariff_classifier.run(text[:4000])
            results.append(ClassificationOutput(
                speech_id=speech_id,
                classification=result.data,
                model_version=tariff_classifier.model.model_name,
            ))
        except Exception as e:
            context.log.warning(f"Classification failed for {speech_id}: {e}")

    context.log.info(f"Classified {len(results)} speeches")
    return results
```

---

## Composition over Inheritance

### Prefer Composition

Build functionality through composition rather than deep inheritance hierarchies:

```python
# GOOD: Composition
class EmbeddingPipeline:
    """Pipeline for generating and storing embeddings."""

    def __init__(
        self,
        embedder: NIMEmbeddingResource,
        storage: WeaviateResource,
        batch_size: int = 32,
    ) -> None:
        self.embedder = embedder
        self.storage = storage
        self.batch_size = batch_size

    def process(self, texts: list[str]) -> list[str]:
        """Generate embeddings and store them."""
        embeddings = self.embedder.embed_texts(texts)
        ids = self.storage.store_embeddings(embeddings)
        return ids

# BAD: Deep inheritance
class BaseEmbedder:
    ...

class NIMEmbedder(BaseEmbedder):
    ...

class BatchNIMEmbedder(NIMEmbedder):
    ...

class StoringBatchNIMEmbedder(BatchNIMEmbedder):
    ...
```

### Protocol Classes for Interfaces

Use Protocol for interface definitions when needed:

```python
from typing import Protocol

class Embedder(Protocol):
    """Protocol for embedding services."""

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for texts."""
        ...

class VectorStore(Protocol):
    """Protocol for vector storage backends."""

    def store(self, embeddings: list[list[float]], metadata: list[dict]) -> list[str]:
        """Store embeddings and return IDs."""
        ...

    def search(self, query_embedding: list[float], limit: int) -> list[SearchResult]:
        """Search for similar embeddings."""
        ...

# Functions accept protocols, not concrete types
def build_index(
    texts: list[str],
    embedder: Embedder,
    store: VectorStore,
) -> list[str]:
    """Build a vector index from texts."""
    embeddings = embedder.embed_texts(texts)
    return store.store(embeddings, [{"text": t} for t in texts])
```

### Factory Functions

Use factory functions instead of complex constructors:

```python
def create_nim_resource(
    endpoint: str | None = None,
    model: str | None = None,
) -> NIMEmbeddingResource:
    """Create a NIM embedding resource with defaults from environment.

    Args:
        endpoint: Override endpoint (defaults to env var or cluster service).
        model: Override model name (defaults to nemoretriever-300m).

    Returns:
        Configured NIMEmbeddingResource.
    """
    return NIMEmbeddingResource(
        endpoint=endpoint or os.getenv(
            "NIM_EMBEDDING_ENDPOINT",
            "http://nvidia-nim-embedding.nvidia-nim.svc.cluster.local:8000"
        ),
        model=model or "nvidia/llama-3_2-nemoretriever-300m-embed-v2",
    )
```

---

## Docstrings

### Google Style Docstrings

Use Google-style docstrings for all public functions, classes, and modules:

```python
def embed_speeches(
    speeches: list[Speech],
    embedder: NIMEmbeddingResource,
    batch_size: int = 32,
) -> list[EmbeddingResult]:
    """Generate embeddings for a collection of speeches.

    Processes speeches in batches to optimize throughput and memory usage.
    Uses the configured NIM embedding model (default: nemoretriever-300m).

    Args:
        speeches: List of Speech objects to embed.
        embedder: NIM embedding resource for generating vectors.
        batch_size: Number of speeches to process per batch. Larger batches
            are more efficient but use more memory.

    Returns:
        List of EmbeddingResult objects containing the speech ID and
        corresponding 1024-dimensional embedding vector.

    Raises:
        EmbeddingError: If the NIM service is unavailable or returns an error.
        ValueError: If speeches list is empty.

    Example:
        >>> embedder = NIMEmbeddingResource()
        >>> speeches = [Speech(speech_id="1", text="...", ...)]
        >>> results = embed_speeches(speeches, embedder)
        >>> len(results[0].embedding)
        1024
    """
    ...
```

### Class Docstrings

Document class purpose, attributes, and usage:

```python
class WeaviateResource(ConfigurableResource):
    """Dagster resource for Weaviate vector database operations.

    Provides methods for creating collections, storing embeddings, and
    performing vector similarity searches. Configured for the Brev Data
    Platform's Weaviate deployment.

    Attributes:
        host: Weaviate server hostname.
        port: HTTP port for REST API.
        grpc_port: gRPC port for vector operations.

    Example:
        ```python
        weaviate = WeaviateResource(
            host="weaviate.weaviate.svc.cluster.local",
            port=8080,
        )

        # Create a collection
        weaviate.create_collection("Speeches", schema)

        # Store embeddings
        weaviate.batch_insert("Speeches", data, embeddings)

        # Search
        results = weaviate.vector_search("Speeches", query_embedding, limit=10)
        ```
    """

    host: str = Field(...)
    port: int = Field(default=8080)
    grpc_port: int = Field(default=50051)
```

### Asset Docstrings

Dagster assets should have clear docstrings explaining their purpose:

```python
@dg.asset(
    description="Central bank speeches with generated embeddings",
    group_name="central_bank_speeches",
)
def speech_embeddings(
    context: dg.AssetExecutionContext,
    cleaned_speeches: pl.DataFrame,
    nim_embedding: NIMEmbeddingResource,
) -> tuple[pl.DataFrame, list[list[float]]]:
    """Generate vector embeddings for all cleaned speeches.

    Uses the local NIM embedding model (llama-3.2-nemoretriever-300m) to
    generate 1024-dimensional embeddings for each speech. The embeddings
    are computed from the concatenation of title and first 2000 characters
    of the speech text.

    This asset is a prerequisite for:
    - weaviate_index: Stores embeddings for vector search
    - speeches_data_product: Final enriched data product

    Args:
        context: Dagster execution context for logging.
        cleaned_speeches: DataFrame with cleaned speech data.
        nim_embedding: NIM embedding service resource.

    Returns:
        Tuple of (original DataFrame, list of embedding vectors).
        The embeddings list has the same order as DataFrame rows.
    """
    ...
```

---

## Linting and Validation

### Required Tools

All code must pass these checks:

```bash
# Type checking with mypy (strict mode)
mypy dagster/src/ --strict

# Linting with ruff
ruff check dagster/src/

# Formatting with ruff
ruff format dagster/src/

# Tests with pytest
pytest dagster/tests/ -v
```

### pyproject.toml Configuration

```toml
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_ignores = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
plugins = ["pydantic.mypy"]

[tool.ruff]
target-version = "py311"
line-length = 100

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # pyflakes
    "I",      # isort
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "UP",     # pyupgrade
    "ARG",    # flake8-unused-arguments
    "SIM",    # flake8-simplify
    "TCH",    # flake8-type-checking
    "ANN",    # flake8-annotations
    "D",      # pydocstyle
]

[tool.ruff.lint.pydocstyle]
convention = "google"
```

---

## Summary Checklist

Before submitting any Dagster code:

- [ ] All functions have complete type annotations (parameters and return types)
- [ ] Complex data structures use Pydantic v2 models
- [ ] LLM processing steps use PydanticAI with structured outputs
- [ ] No deep inheritance hierarchies (use composition)
- [ ] All public functions/classes have Google-style docstrings
- [ ] `mypy --strict` passes with no errors
- [ ] `ruff check` passes with no errors
- [ ] `ruff format` has been applied
- [ ] All tests pass
- [ ] Assets use I/O managers (no direct storage calls)
- [ ] Resources use `ConfigurableResource` (Pydantic-based)
- [ ] No hardcoded credentials (use `EnvVar` or environment variables)
