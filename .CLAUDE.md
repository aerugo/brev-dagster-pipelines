# Dagster Pipeline Development Best Practices

This document defines coding standards for all Dagster pipeline code in the Brev Data Platform. These guidelines ensure consistency, maintainability, and type safety.

> **Essential Reading**: Review `docs/invariants/INVARIANTS.md` for architectural invariants that must never be violated.

## Core Principles

1. **Strict Type Hints**: All functions, assets, and resources must have complete type annotations
2. **Pydantic for Data Models**: Use Pydantic v2 API for all structured data
3. **PydanticAI for LLM Steps**: Use PydanticAI for LLM-based processing with structured outputs
4. **Composition over Inheritance**: Build functionality through composition, not class hierarchies
5. **Comprehensive Docstrings**: All public functions and classes must have docstrings
6. **Test-Driven Development**: Write tests BEFORE implementation

---

## Key Invariants

These invariants from `docs/invariants/INVARIANTS.md` must never be violated:

- **INV-I006**: Local-only infrastructure - NEVER use cloud APIs (OpenAI, Anthropic, NVIDIA Cloud)
- **INV-P001**: Assets over ops - use `@asset` for data transformations
- **INV-P002**: I/O managers for storage - no direct storage calls in assets
- **INV-P003**: Type annotations on all assets
- **INV-P008**: PydanticAI for LLM steps with strictly-typed response models
- **INV-D002**: All data through LakeFS - never write directly to MinIO
- **INV-D003**: Parquet for structured data

---

## Type Annotations

### Required Annotations

Every function must have complete type annotations for parameters and return values.

```python
# GOOD: Complete type annotations
def process_speeches(
    speeches: list[Speech],
    batch_size: int = 32,
) -> tuple[pl.DataFrame, list[EmbeddingResult]]:
    ...

# BAD: Missing annotations
def process_speeches(speeches, batch_size=32):
    ...
```

### Use Modern Python Typing

Use Python 3.10+ typing syntax:

```python
# GOOD: Modern syntax
from collections.abc import Sequence

def get_items(ids: list[str]) -> dict[str, Item | None]:
    ...

def process(data: Sequence[Record]) -> list[Result]:
    ...

# BAD: Legacy typing module
from typing import List, Dict, Optional, Union

def get_items(ids: List[str]) -> Dict[str, Optional[Item]]:
    ...
```

### No `Any` Types

Avoid `Any`. Define proper types using Pydantic models or TypedDict:

```python
# FORBIDDEN: leaks unknown types
from typing import Any

def classify_speech(row: dict[str, Any]) -> dict[str, Any]:
    return {"stance": "hawkish", "confidence": 0.9}

# CORRECT: use Pydantic models
from pydantic import BaseModel, Field

class ClassificationResult(BaseModel):
    """Structured classification result."""
    stance: Literal["hawkish", "dovish", "neutral"]
    confidence: float = Field(ge=0.0, le=1.0)
    evidence: list[str] = Field(default_factory=list)

def classify_speech(row: SpeechRecord) -> ClassificationResult:
    ...
```

### TypedDict for Dict Shapes

When you must use dicts (e.g., I/O boundaries), define their shape:

```python
from typing import TypedDict

class SpeechDict(TypedDict):
    reference: str
    title: str
    text: str
    central_bank: str
    monetary_stance: int
    tariff_mention: bool

class EmbeddingResultDict(TypedDict):
    reference: str
    embedding: list[float]
    model: str

def prepare_for_weaviate(
    speech: SpeechDict,
    embedding: list[float],
) -> dict[str, str | int | bool | list[float]]:
    ...
```

### Type Aliases for Complex Types

Define type aliases for complex or repeated types:

```python
# Define at module level
EmbeddingVector = list[float]
BatchResult = tuple[pl.DataFrame, list[EmbeddingVector]]
ClassificationScale = Literal[1, 2, 3, 4, 5]
StanceLabel = Literal["very_dovish", "somewhat_dovish", "neutral", "somewhat_hawkish", "very_hawkish"]

@dg.asset
def speech_embeddings(
    speeches: pl.DataFrame,
    nim_embedding: NIMEmbeddingResource,
) -> BatchResult:
    ...
```

---

## Pydantic Models (v2 API)

### Model Definition

Use Pydantic v2 for all structured data with proper field definitions:

```python
from pydantic import BaseModel, Field, field_validator
from datetime import date

class Speech(BaseModel):
    """A central bank speech record."""

    speech_id: str = Field(..., description="Unique identifier for the speech")
    title: str = Field(..., min_length=1, description="Speech title")
    date: date = Field(..., description="Date of the speech")
    central_bank: str = Field(..., description="Issuing central bank")
    speaker: str | None = Field(default=None, description="Speaker name if known")
    text: str = Field(..., min_length=10, description="Full speech text")
    tariff_mention: bool = Field(default=False, description="Whether speech mentions tariffs")

    @field_validator("central_bank")
    @classmethod
    def validate_central_bank(cls, v: str) -> str:
        """Normalize central bank names."""
        return v.strip().upper()

class SpeechCollection(BaseModel):
    """Collection of speeches with metadata."""

    speeches: list[Speech]
    source: str = Field(..., description="Data source identifier")
    extracted_at: datetime = Field(default_factory=datetime.utcnow)

    @property
    def count(self) -> int:
        return len(self.speeches)
```

### Dagster Resources with Pydantic

Use `ConfigurableResource` (Pydantic-based) for Dagster resources:

```python
from dagster import ConfigurableResource
from pydantic import Field, SecretStr

class NIMEmbeddingResource(ConfigurableResource):
    """Resource for generating embeddings via local NIM service."""

    endpoint: str = Field(
        default="http://nvidia-nim-embedding.nvidia-nim.svc.cluster.local:8000",
        description="NIM embedding service endpoint",
    )
    model: str = Field(
        default="nvidia/llama-3_2-nemoretriever-300m-embed-v2",
        description="Embedding model name",
    )
    timeout: int = Field(default=30, ge=1, le=300, description="Request timeout in seconds")
    batch_size: int = Field(default=32, ge=1, le=256, description="Batch size for embedding requests")

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of text strings to embed.

        Returns:
            List of embedding vectors (1024 dimensions each).

        Raises:
            EmbeddingError: If the embedding request fails.
        """
        ...
```

### Validation and Serialization

Use Pydantic for input validation and output serialization:

```python
class ClassificationResult(BaseModel):
    """Result of LLM classification."""

    speech_id: str
    classification: Literal["tariff_related", "not_tariff_related"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str | None = None

    model_config = ConfigDict(
        # Pydantic v2 config
        strict=True,
        frozen=True,  # Immutable after creation
    )

def parse_llm_response(response: str, speech_id: str) -> ClassificationResult:
    """Parse and validate LLM classification response."""
    # Pydantic validates automatically
    return ClassificationResult.model_validate_json(response)
```

---

## PydanticAI for LLM Processing

### Why PydanticAI

Use PydanticAI for LLM steps because it provides:
- Structured output validation with Pydantic models
- Automatic retries on validation failures
- Type-safe prompt templates
- Built-in support for multiple LLM providers

**CRITICAL (INV-I006)**: All LLM processing must use **local NVIDIA NIM only**. NEVER use cloud APIs.

### PydanticAI with Local NIM (Required Pattern)

NVIDIA NIM provides an OpenAI-compatible API. Configure PydanticAI using `OpenAIChatModel` with `OpenAIProvider`:

```python
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.openai import OpenAIProvider
from typing import Literal

# Configure for LOCAL NVIDIA NIM (the ONLY allowed pattern)
nim_provider = OpenAIProvider(
    base_url="http://nvidia-nim.nvidia-nim.svc.cluster.local:8000/v1",
    api_key="not-required",  # Local NIM doesn't require API key
)

nim_model = OpenAIChatModel(
    model_name="meta/llama3-8b-instruct",
    provider=nim_provider,
)

# Strictly-typed response model - ALL fields must be typed
class TariffClassification(BaseModel):
    """Classification result for tariff mention detection."""

    mentions_tariff: bool = Field(description="Whether the speech mentions tariffs or trade policy")
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence score")
    evidence: list[str] = Field(default_factory=list, max_length=3, description="Relevant quotes")
    stance: Literal["protectionist", "globalist", "neutral"] = Field(
        description="Overall trade policy stance"
    )

# Create agent with typed response
tariff_classifier = Agent(
    model=nim_model,  # LOCAL NIM - never cloud APIs!
    result_type=TariffClassification,  # REQUIRED: ensures typed output
    system_prompt=(
        "You are an expert analyst classifying central bank speeches. "
        "Determine if the speech discusses tariffs, trade policy, or trade tensions. "
        "Extract relevant evidence quotes."
    ),
)

async def classify_speech(text: str) -> TariffClassification:
    """Classify a speech for tariff mentions using PydanticAI."""
    result = await tariff_classifier.run(text[:4000])
    return result.data  # Guaranteed to match schema
```

### FORBIDDEN Patterns (Cloud APIs)

**NEVER use these patterns** - they violate INV-I006:

```python
# NEVER: OpenAI API
from openai import OpenAI
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])  # FORBIDDEN

# NEVER: Anthropic API
from anthropic import Anthropic
client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])  # FORBIDDEN

# NEVER: NVIDIA Cloud API
nim_cloud = OpenAIProvider(
    base_url="https://integrate.api.nvidia.com/v1",  # FORBIDDEN - cloud!
    api_key=os.environ["NVIDIA_API_KEY"],
)

# NEVER: PydanticAI with cloud models
agent = Agent(model="openai:gpt-4o-mini", ...)  # FORBIDDEN
agent = Agent(model="anthropic:claude-3-sonnet", ...)  # FORBIDDEN
```

### PydanticAI in Dagster Assets

Integrate PydanticAI with Dagster assets:

```python
import dagster as dg
from pydantic_ai import Agent

class ClassificationOutput(BaseModel):
    """Output model for classification asset."""

    speech_id: str
    classification: TariffClassification
    model_version: str

@dg.asset(
    description="Classify speeches for tariff mentions using PydanticAI",
    group_name="central_bank_speeches",
)
async def tariff_classifications(
    context: dg.AssetExecutionContext,
    cleaned_speeches: pl.DataFrame,
) -> list[ClassificationOutput]:
    """Classify all speeches using PydanticAI agent.

    Uses structured output to ensure consistent classification format.
    """
    results: list[ClassificationOutput] = []

    for row in cleaned_speeches.iter_rows(named=True):
        speech_id = row["speech_id"]
        text = row["text"]

        try:
            result = await tariff_classifier.run(text[:4000])
            results.append(ClassificationOutput(
                speech_id=speech_id,
                classification=result.data,
                model_version=tariff_classifier.model.model_name,
            ))
        except Exception as e:
            context.log.warning(f"Classification failed for {speech_id}: {e}")

    context.log.info(f"Classified {len(results)} speeches")
    return results
```

---

## Composition over Inheritance

### Prefer Composition

Build functionality through composition rather than deep inheritance hierarchies:

```python
# GOOD: Composition
class EmbeddingPipeline:
    """Pipeline for generating and storing embeddings."""

    def __init__(
        self,
        embedder: NIMEmbeddingResource,
        storage: WeaviateResource,
        batch_size: int = 32,
    ) -> None:
        self.embedder = embedder
        self.storage = storage
        self.batch_size = batch_size

    def process(self, texts: list[str]) -> list[str]:
        """Generate embeddings and store them."""
        embeddings = self.embedder.embed_texts(texts)
        ids = self.storage.store_embeddings(embeddings)
        return ids

# BAD: Deep inheritance
class BaseEmbedder:
    ...

class NIMEmbedder(BaseEmbedder):
    ...

class BatchNIMEmbedder(NIMEmbedder):
    ...

class StoringBatchNIMEmbedder(BatchNIMEmbedder):
    ...
```

### Protocol Classes for Interfaces

Use Protocol for interface definitions. Implementations don't need to inherit:

```python
from typing import Protocol, runtime_checkable

@runtime_checkable
class Embedder(Protocol):
    """Protocol for embedding services."""

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for texts."""
        ...

@runtime_checkable
class VectorStore(Protocol):
    """Protocol for vector storage backends."""

    def store(
        self,
        embeddings: list[list[float]],
        metadata: list[dict[str, str | int]],
    ) -> list[str]:
        """Store embeddings and return IDs."""
        ...

    def search(self, query_embedding: list[float], limit: int) -> list[SearchResult]:
        """Search for similar embeddings."""
        ...

# Implementation - plain class, no inheritance needed
class NIMEmbedder:
    def __init__(self, endpoint: str, model: str) -> None:
        self.endpoint = endpoint
        self.model = model

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        ...

# Functions accept protocols, not concrete types
def build_index(
    texts: list[str],
    embedder: Embedder,
    store: VectorStore,
) -> list[str]:
    """Build a vector index from texts."""
    embeddings = embedder.embed_texts(texts)
    return store.store(embeddings, [{"text": t} for t in texts])
```

### Factory Functions

Use factory functions instead of complex constructors:

```python
def create_nim_resource(
    endpoint: str | None = None,
    model: str | None = None,
) -> NIMEmbeddingResource:
    """Create a NIM embedding resource with defaults from environment.

    Args:
        endpoint: Override endpoint (defaults to env var or cluster service).
        model: Override model name (defaults to nemoretriever-300m).

    Returns:
        Configured NIMEmbeddingResource.
    """
    return NIMEmbeddingResource(
        endpoint=endpoint or os.getenv(
            "NIM_EMBEDDING_ENDPOINT",
            "http://nvidia-nim-embedding.nvidia-nim.svc.cluster.local:8000"
        ),
        model=model or "nvidia/llama-3_2-nemoretriever-300m-embed-v2",
    )
```

---

## Docstrings

### Google Style Docstrings

Use Google-style docstrings for all public functions, classes, and modules:

```python
def embed_speeches(
    speeches: list[Speech],
    embedder: NIMEmbeddingResource,
    batch_size: int = 32,
) -> list[EmbeddingResult]:
    """Generate embeddings for a collection of speeches.

    Processes speeches in batches to optimize throughput and memory usage.
    Uses the configured NIM embedding model (default: nemoretriever-300m).

    Args:
        speeches: List of Speech objects to embed.
        embedder: NIM embedding resource for generating vectors.
        batch_size: Number of speeches to process per batch. Larger batches
            are more efficient but use more memory.

    Returns:
        List of EmbeddingResult objects containing the speech ID and
        corresponding 1024-dimensional embedding vector.

    Raises:
        EmbeddingError: If the NIM service is unavailable or returns an error.
        ValueError: If speeches list is empty.

    Example:
        >>> embedder = NIMEmbeddingResource()
        >>> speeches = [Speech(speech_id="1", text="...", ...)]
        >>> results = embed_speeches(speeches, embedder)
        >>> len(results[0].embedding)
        1024
    """
    ...
```

### Class Docstrings

Document class purpose, attributes, and usage:

```python
class WeaviateResource(ConfigurableResource):
    """Dagster resource for Weaviate vector database operations.

    Provides methods for creating collections, storing embeddings, and
    performing vector similarity searches. Configured for the Brev Data
    Platform's Weaviate deployment.

    Attributes:
        host: Weaviate server hostname.
        port: HTTP port for REST API.
        grpc_port: gRPC port for vector operations.

    Example:
        ```python
        weaviate = WeaviateResource(
            host="weaviate.weaviate.svc.cluster.local",
            port=8080,
        )

        # Create a collection
        weaviate.create_collection("Speeches", schema)

        # Store embeddings
        weaviate.batch_insert("Speeches", data, embeddings)

        # Search - returns list[WeaviateSearchResult]
        results = weaviate.vector_search("Speeches", query_embedding, limit=10)
        # Access properties: result["properties"]["field_name"]
        # Access metadata: result["_distance"], result["_certainty"]
        ```
    """

    host: str = Field(...)
    port: int = Field(default=8080)
    grpc_port: int = Field(default=50051)
```

### Asset Docstrings

Dagster assets should have clear docstrings explaining their purpose:

```python
@dg.asset(
    description="Central bank speeches with generated embeddings",
    group_name="central_bank_speeches",
)
def speech_embeddings(
    context: dg.AssetExecutionContext,
    cleaned_speeches: pl.DataFrame,
    nim_embedding: NIMEmbeddingResource,
) -> tuple[pl.DataFrame, list[list[float]]]:
    """Generate vector embeddings for all cleaned speeches.

    Uses the local NIM embedding model (llama-3.2-nemoretriever-300m) to
    generate 1024-dimensional embeddings for each speech. The embeddings
    are computed from the concatenation of title and first 2000 characters
    of the speech text.

    This asset is a prerequisite for:
    - weaviate_index: Stores embeddings for vector search
    - speeches_data_product: Final enriched data product

    Args:
        context: Dagster execution context for logging.
        cleaned_speeches: DataFrame with cleaned speech data.
        nim_embedding: NIM embedding service resource.

    Returns:
        Tuple of (original DataFrame, list of embedding vectors).
        The embeddings list has the same order as DataFrame rows.
    """
    ...
```

---

## Linting and Validation

### Required Tools

All code must pass these checks:

```bash
cd dagster

# Type checking with mypy (strict mode)
uv run mypy src/ --strict

# Type checking with pyright (matches VS Code Pylance)
uv run pyright src/

# Linting with ruff
uv run ruff check src/

# Formatting with ruff
uv run ruff format src/ --check

# Tests with pytest
uv run pytest tests/ -v
```

**Note**: Both mypy and pyright must pass. They catch different issues - if VS Code shows an error, pyright will too.

### pyproject.toml Configuration

```toml
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_ignores = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
plugins = ["pydantic.mypy"]

[tool.ruff]
target-version = "py311"
line-length = 100

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # pyflakes
    "I",      # isort
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "UP",     # pyupgrade
    "ARG",    # flake8-unused-arguments
    "SIM",    # flake8-simplify
    "TCH",    # flake8-type-checking
    "ANN",    # flake8-annotations
    "D",      # pydocstyle
]

[tool.ruff.lint.pydocstyle]
convention = "google"
```

---

## Test-Driven Development (TDD)

**CRITICAL**: Follow TDD for all new code. Write tests BEFORE implementation.

### TDD Workflow

1. **Write the test first** - Define expected behavior
2. **Run test (should fail)** - Verify test catches missing functionality
3. **Write minimal code** - Just enough to pass the test
4. **Run test (should pass)** - Verify implementation works
5. **Refactor** - Clean up while keeping tests green

### Testing Pydantic Models

```python
# dagster/tests/unit/test_models.py
"""Tests for Pydantic models."""
import pytest
from pydantic import ValidationError

from brev_pipelines.models.speech import Speech


class TestSpeechModel:
    """Tests for Speech Pydantic model."""

    def test_valid_speech(self) -> None:
        """Test creating a valid Speech instance."""
        speech = Speech(
            speech_id="BIS_2024_001",
            title="Monetary Policy",
            text="The central bank..." * 10,
            central_bank="FED",
        )
        assert speech.speech_id == "BIS_2024_001"

    def test_monetary_stance_bounds(self) -> None:
        """Test monetary_stance must be 1-5."""
        with pytest.raises(ValidationError):
            Speech(
                speech_id="1",
                title="Test",
                text="x" * 100,
                central_bank="FED",
                monetary_stance=6,  # Invalid
            )
```

### Testing Resources

```python
# dagster/tests/unit/test_resources.py
"""Tests for Dagster resources."""
from unittest.mock import Mock, patch

from brev_pipelines.resources.nim_embedding import NIMEmbeddingResource


class TestNIMEmbeddingResource:
    """Tests for NIM embedding resource."""

    def test_initialization_defaults(self) -> None:
        """Test resource initializes with defaults."""
        resource = NIMEmbeddingResource()
        assert resource.dimensions == 1024
        assert resource.timeout == 120

    @patch("requests.post")
    def test_embed_texts_success(self, mock_post: Mock) -> None:
        """Test successful embedding generation."""
        mock_post.return_value.json.return_value = {
            "data": [{"embedding": [0.1] * 1024}]
        }
        mock_post.return_value.raise_for_status = Mock()

        resource = NIMEmbeddingResource(endpoint="http://test:8000")
        embeddings = resource.embed_texts(["text"])

        assert len(embeddings) == 1
        assert len(embeddings[0]) == 1024
```

### Testing Assets

```python
# dagster/tests/unit/test_assets.py
"""Tests for Dagster assets."""
import polars as pl
from dagster import build_asset_context

from brev_pipelines.assets.central_bank_speeches import cleaned_speeches


class TestCleanedSpeeches:
    """Tests for cleaned_speeches asset."""

    def test_filters_empty_speeches(self) -> None:
        """Test that speeches with <100 chars are filtered."""
        raw_data = pl.DataFrame({
            "reference": ["1", "2"],
            "text": ["short", "x" * 150],
            "title": ["A", "B"],
        })

        context = build_asset_context()
        result = cleaned_speeches(context, raw_data)

        assert len(result) == 1
        assert "1" not in result["reference"].to_list()
```

### Test Fixtures

```python
# dagster/tests/conftest.py
"""Shared test fixtures."""
import pytest
import polars as pl
from unittest.mock import MagicMock


@pytest.fixture
def sample_speeches_df() -> pl.DataFrame:
    """Create sample speeches DataFrame."""
    return pl.DataFrame({
        "reference": ["REF001", "REF002"],
        "title": ["Speech 1", "Speech 2"],
        "text": ["Policy text..." * 50, "Economic text..." * 50],
        "central_bank": ["FED", "ECB"],
    })


@pytest.fixture
def mock_nim_embedding() -> MagicMock:
    """Create mock NIM embedding resource."""
    mock = MagicMock()
    mock.embed_texts.return_value = [[0.1] * 1024]
    mock.dimensions = 1024
    return mock
```

---

## Project Structure

```
dagster/
├── .CLAUDE.md                   # This file - coding guidelines
├── pyproject.toml               # Dependencies and tool config
├── src/
│   └── brev_pipelines/
│       ├── __init__.py
│       ├── definitions.py       # Dagster Definitions entry point
│       ├── config.py            # Pipeline configuration
│       ├── models/              # Pydantic models
│       │   ├── __init__.py
│       │   └── speech.py
│       ├── assets/
│       │   ├── __init__.py
│       │   ├── central_bank_speeches.py
│       │   └── synthetic_speeches.py
│       ├── io_managers/
│       │   ├── __init__.py
│       │   ├── lakefs_polars.py
│       │   └── weaviate_io.py
│       ├── resources/
│       │   ├── __init__.py
│       │   ├── lakefs.py
│       │   ├── minio.py
│       │   ├── nim.py
│       │   ├── nim_embedding.py
│       │   └── weaviate.py
│       └── agents/              # PydanticAI agents
│           ├── __init__.py
│           └── classifier.py
└── tests/
    ├── __init__.py
    ├── conftest.py              # Shared fixtures
    ├── unit/
    │   ├── test_models.py
    │   ├── test_resources.py
    │   └── test_assets.py
    ├── integration/
    │   └── test_pipeline.py
    └── property/
        └── test_invariants.py
```

---

## Summary Checklist

Before submitting any Dagster code:

### Type Safety
- [ ] All functions have complete type annotations (parameters AND return types)
- [ ] No `Any` types - use Pydantic models or TypedDict
- [ ] No bare generics (`list`, `dict`) - always specify contents
- [ ] Complex data structures use Pydantic v2 models
- [ ] LLM processing steps use PydanticAI with structured outputs

### Code Quality
- [ ] No deep inheritance hierarchies (use composition)
- [ ] All public functions/classes have Google-style docstrings
- [ ] Assets use I/O managers (no direct storage calls)
- [ ] Resources use `ConfigurableResource` (Pydantic-based)
- [ ] No hardcoded credentials (use `EnvVar` or environment variables)

### Testing
- [ ] Tests written BEFORE implementation (TDD)
- [ ] Unit tests for all Pydantic models
- [ ] Unit tests for all resources
- [ ] Unit tests for all assets
- [ ] External services mocked in tests

### Validation
- [ ] `uv run mypy src/ --strict` passes
- [ ] `uv run pyright src/` passes
- [ ] `uv run ruff check src/` passes
- [ ] `uv run ruff format src/ --check` passes
- [ ] `uv run pytest tests/` passes
